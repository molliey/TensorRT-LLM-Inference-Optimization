# Triton Inference Server model configuration for TensorRT-LLM GPT2
name: "gpt2_tensorrt_llm"
backend: "tensorrtllm"
max_batch_size: 8

# Model instance configuration
instance_group [
  {
    count: 1
    kind: KIND_GPU
    gpus: [0]
  }
]

# Input specifications
input [
  {
    name: "input_ids"
    data_type: TYPE_INT32
    dims: [-1]  # Variable sequence length
    allow_ragged_batch: true
  },
  {
    name: "input_lengths"
    data_type: TYPE_INT32
    dims: [1]
    reshape: { shape: [] }
  },
  {
    name: "request_output_len"
    data_type: TYPE_INT32
    dims: [1]
    reshape: { shape: [] }
  },
  {
    name: "temperature"
    data_type: TYPE_FP32
    dims: [1]
    reshape: { shape: [] }
    optional: true
  },
  {
    name: "top_k"
    data_type: TYPE_INT32
    dims: [1]
    reshape: { shape: [] }
    optional: true
  },
  {
    name: "top_p"
    data_type: TYPE_FP32
    dims: [1]
    reshape: { shape: [] }
    optional: true
  },
  {
    name: "beam_width"
    data_type: TYPE_INT32
    dims: [1]
    reshape: { shape: [] }
    optional: true
  },
  {
    name: "repetition_penalty"
    data_type: TYPE_FP32
    dims: [1]
    reshape: { shape: [] }
    optional: true
  },
  {
    name: "presence_penalty"
    data_type: TYPE_FP32
    dims: [1]
    reshape: { shape: [] }
    optional: true
  },
  {
    name: "frequency_penalty"
    data_type: TYPE_FP32
    dims: [1]
    reshape: { shape: [] }
    optional: true
  },
  {
    name: "random_seed"
    data_type: TYPE_UINT64
    dims: [1]
    reshape: { shape: [] }
    optional: true
  },
  {
    name: "return_log_probs"
    data_type: TYPE_BOOL
    dims: [1]
    reshape: { shape: [] }
    optional: true
  },
  {
    name: "streaming"
    data_type: TYPE_BOOL
    dims: [1]
    reshape: { shape: [] }
    optional: true
  },
  {
    name: "end_id"
    data_type: TYPE_INT32
    dims: [1]
    reshape: { shape: [] }
    optional: true
  },
  {
    name: "pad_id"
    data_type: TYPE_INT32
    dims: [1]
    reshape: { shape: [] }
    optional: true
  },
  {
    name: "stop_words_list"
    data_type: TYPE_INT32
    dims: [-1, -1]
    optional: true
    allow_ragged_batch: true
  },
  {
    name: "bad_words_list"
    data_type: TYPE_INT32
    dims: [-1, -1]
    optional: true
    allow_ragged_batch: true
  }
]

# Output specifications
output [
  {
    name: "output_ids"
    data_type: TYPE_INT32
    dims: [-1, -1]
  },
  {
    name: "sequence_length"
    data_type: TYPE_INT32
    dims: [-1]
  },
  {
    name: "output_log_probs"
    data_type: TYPE_FP32
    dims: [-1, -1]
  },
  {
    name: "cum_log_probs"
    data_type: TYPE_FP32
    dims: [-1]
  }
]

# Model parameters
parameters: {
  key: "gpt_model_type"
  value: {
    string_value: "gpt"
  }
}

parameters: {
  key: "gpt_model_path"
  value: {
    string_value: "/app/engines/gpt2_optimized"
  }
}

parameters: {
  key: "max_tokens_in_paged_kv_cache"
  value: {
    string_value: "8192"
  }
}

parameters: {
  key: "max_attention_window_size"
  value: {
    string_value: "1024"
  }
}

parameters: {
  key: "sink_token_length"
  value: {
    string_value: "4"
  }
}

parameters: {
  key: "batch_scheduler_policy"
  value: {
    string_value: "max_utilization"
  }
}

parameters: {
  key: "kv_cache_free_gpu_mem_fraction"
  value: {
    string_value: "0.85"
  }
}

parameters: {
  key: "enable_trt_overlap"
  value: {
    string_value: "true"
  }
}

parameters: {
  key: "exclude_input_in_output"
  value: {
    string_value: "true"
  }
}

parameters: {
  key: "enable_kv_cache_reuse"
  value: {
    string_value: "true"
  }
}

parameters: {
  key: "normalize_log_probs"
  value: {
    string_value: "true"
  }
}

parameters: {
  key: "enable_chunked_context"
  value: {
    string_value: "false"
  }
}

parameters: {
  key: "gpu_device_ids"
  value: {
    string_value: "0"
  }
}

parameters: {
  key: "lora_cache_optimal_adapter_size"
  value: {
    string_value: "8"
  }
}

parameters: {
  key: "lora_cache_max_adapter_size"
  value: {
    string_value: "64"
  }
}

parameters: {
  key: "lora_cache_gpu_memory_fraction"
  value: {
    string_value: "0.05"
  }
}

parameters: {
  key: "max_queue_delay_microseconds"
  value: {
    string_value: "1000"
  }
}

parameters: {
  key: "request_cache_enable"
  value: {
    string_value: "true"
  }
}

parameters: {
  key: "request_cache_max_size"
  value: {
    string_value: "128"
  }
}

# Dynamic batching configuration
dynamic_batching {
  max_queue_delay_microseconds: 1000
  preferred_batch_size: [4, 8]
  preserve_ordering: false
}

# Model warmup
model_warmup [
  {
    name: "sample_request"
    batch_size: 1
    inputs: {
      key: "input_ids"
      value: {
        data_type: TYPE_INT32
        dims: [5]
        int_contents: [15496, 11, 616, 389, 345]  # "Hello, how are you"
      }
    }
    inputs: {
      key: "input_lengths"
      value: {
        data_type: TYPE_INT32
        dims: [1]
        int_contents: [5]
      }
    }
    inputs: {
      key: "request_output_len"
      value: {
        data_type: TYPE_INT32
        dims: [1]
        int_contents: [20]
      }
    }
  }
]